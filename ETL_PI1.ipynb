{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL: Preparación de datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracción:  \n",
    "En primera instancia obtendremos los datasets en crudo del siguiente repositorio: https://github.com/soyHenry/fe-ct-pimlops2  \n",
    "O de la siguiente página: https://drive.google.com/drive/folders/1X_LdCoGTHJDbD28_dJTxaD4fVuQC9Wt5  \n",
    "De estos links obtenremos dos archivos en formato .csv.  \n",
    "Veremos paso a paso una guía breve para realizar una limpieza y obtener los datasets aptos para subir a un repositorio de github y que puedan ser consumidos por nuestra API a través de Render.\n",
    "  \n",
    "Una vez los archivos en nuestra computadora, procedemos a importar los archivos y a comenzar la transformación.  \n",
    "  \n",
    "Transformación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_11364\\1383607456.py:7: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_movies = pd.read_csv('C:/Users/Usuario/Desktop/Data Science/SoyHenry/PI1_files/Datasets/movies_dataset.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Cargar el CSV en un DataFrame\n",
    "df_movies = pd.read_csv('Datasets/movies_dataset.csv')\n",
    "df_credits =pd.read_csv('Datasets/credits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45466 entries, 0 to 45465\n",
      "Data columns (total 24 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   adult                  45466 non-null  object \n",
      " 1   belongs_to_collection  4494 non-null   object \n",
      " 2   budget                 45466 non-null  object \n",
      " 3   genres                 45466 non-null  object \n",
      " 4   homepage               7782 non-null   object \n",
      " 5   id                     45466 non-null  object \n",
      " 6   imdb_id                45449 non-null  object \n",
      " 7   original_language      45455 non-null  object \n",
      " 8   original_title         45466 non-null  object \n",
      " 9   overview               44512 non-null  object \n",
      " 10  popularity             45461 non-null  object \n",
      " 11  poster_path            45080 non-null  object \n",
      " 12  production_companies   45463 non-null  object \n",
      " 13  production_countries   45463 non-null  object \n",
      " 14  release_date           45379 non-null  object \n",
      " 15  revenue                45460 non-null  float64\n",
      " 16  runtime                45203 non-null  float64\n",
      " 17  spoken_languages       45460 non-null  object \n",
      " 18  status                 45379 non-null  object \n",
      " 19  tagline                20412 non-null  object \n",
      " 20  title                  45460 non-null  object \n",
      " 21  video                  45460 non-null  object \n",
      " 22  vote_average           45460 non-null  float64\n",
      " 23  vote_count             45460 non-null  float64\n",
      "dtypes: float64(4), object(20)\n",
      "memory usage: 8.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Visualizamos las dimensiones del dataframe movies, sus tipos, sus nulos y su peso.\n",
    "print(df_movies.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45476 entries, 0 to 45475\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   cast    45476 non-null  object\n",
      " 1   crew    45476 non-null  object\n",
      " 2   id      45476 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Visualizamos las dimensiones del dataframe credits, sus tipos, sus nulos y su peso.\n",
    "print(df_credits.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cast</th>\n",
       "      <th>crew</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'cast_id': 14, 'character': 'Woody (voice)',...</td>\n",
       "      <td>[{'credit_id': '52fe4284c3a36847f8024f49', 'de...</td>\n",
       "      <td>862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'cast_id': 1, 'character': 'Alan Parrish', '...</td>\n",
       "      <td>[{'credit_id': '52fe44bfc3a36847f80a7cd1', 'de...</td>\n",
       "      <td>8844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'cast_id': 2, 'character': 'Max Goldman', 'c...</td>\n",
       "      <td>[{'credit_id': '52fe466a9251416c75077a89', 'de...</td>\n",
       "      <td>15602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'cast_id': 1, 'character': \"Savannah 'Vannah...</td>\n",
       "      <td>[{'credit_id': '52fe44779251416c91011acb', 'de...</td>\n",
       "      <td>31357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'cast_id': 1, 'character': 'George Banks', '...</td>\n",
       "      <td>[{'credit_id': '52fe44959251416c75039ed7', 'de...</td>\n",
       "      <td>11862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                cast  \\\n",
       "0  [{'cast_id': 14, 'character': 'Woody (voice)',...   \n",
       "1  [{'cast_id': 1, 'character': 'Alan Parrish', '...   \n",
       "2  [{'cast_id': 2, 'character': 'Max Goldman', 'c...   \n",
       "3  [{'cast_id': 1, 'character': \"Savannah 'Vannah...   \n",
       "4  [{'cast_id': 1, 'character': 'George Banks', '...   \n",
       "\n",
       "                                                crew     id  \n",
       "0  [{'credit_id': '52fe4284c3a36847f8024f49', 'de...    862  \n",
       "1  [{'credit_id': '52fe44bfc3a36847f80a7cd1', 'de...   8844  \n",
       "2  [{'credit_id': '52fe466a9251416c75077a89', 'de...  15602  \n",
       "3  [{'credit_id': '52fe44779251416c91011acb', 'de...  31357  \n",
       "4  [{'credit_id': '52fe44959251416c75039ed7', 'de...  11862  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_credits.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos una lista con objetivos que cumplir en esta etapa de desarrollo del proyecto:\n",
    "\n",
    "1) Desanidado de columnas:  \n",
    "Identificar qué columnas están anidadas: belongs_to_collection, production_companies, genres, production_countries y spoken language. También será necesario desanidar el dataset Credits.\n",
    "\n",
    "2) Tratamiento de valores nulos:  \n",
    "Nulos cambiados por cero: Revenue y Budget.\n",
    "Nulos a eliminar: Release.\n",
    "\n",
    "3) Tratamiento de fechas:  \n",
    "Formato pedido: AAAA-mm-dd  \n",
    "  \n",
    "4) Creación de columnas nuevas:  \n",
    "Creación de columna release_year: año de la fecha de estreno.  \n",
    "Creación de columna return: Retorno de inversión; revenue / budget (si no hay datos disponibles para calcularlo, deberá tomar el valor 0)\n",
    "\n",
    "5) Eliminación de columnas:  \n",
    "Video, imdb_id, adult, original_title, poster_path y homepage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo CSV con columnas expandidas guardado como 'movies_dataset_transformado_2.csv'\n"
     ]
    }
   ],
   "source": [
    "# 1) DESANIDADO DE COLUMNAS:\n",
    "\n",
    "# Probamos técnicas para desanidado con una sola columna en un principio.\n",
    "column_name = 'genres'\n",
    "\n",
    "# Definimos una función que maneje NaN y cadenas mal formateadas\n",
    "def safe_literal_eval(val):\n",
    "    try:\n",
    "        if pd.isna(val):\n",
    "            return []\n",
    "        return ast.literal_eval(val)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []\n",
    "\n",
    "# Aplicamos la función a la columna para convertirla a listas de diccionarios\n",
    "df_movies[column_name] = df_movies[column_name].apply(safe_literal_eval)\n",
    "\n",
    "# Extraemos las IDs y Names en nuevas columnas\n",
    "df_movies['genres_id'] = df_movies[column_name].apply(lambda x: [d['id'] for d in x])\n",
    "df_movies['genres_name'] = df_movies[column_name].apply(lambda x: [d['name'] for d in x])\n",
    "\n",
    "# Eliminamos la columna original si ya no es necesaria\n",
    "df_movies.drop(columns=['genres'], inplace=True)\n",
    "\n",
    "# Guardamos el resultado en un nuevo archivo CSV\n",
    "#df_movies.to_csv('movies_dataset_transformado_2.csv', index=False)\n",
    "\n",
    "print(\"Archivo CSV con columnas expandidas guardado como 'movies_dataset_transformado_2.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_3908\\136982011.py:9: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if pd.isna(val):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores inesperados en la columna production_companies:\n",
      "19730    False\n",
      "29503    False\n",
      "35587    False\n",
      "Name: production_companies, dtype: object\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Extraemos las claves correspondientes y crear nuevas columnas\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys:\n\u001b[1;32m---> 22\u001b[0m         df_movies[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_movies\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnan\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Guardamos el resultado en un nuevo archivo CSV\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# df_movies.to_csv('movies_dataset_transformado_final.csv', index=False)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArchivo CSV con columnas expandidas guardado como \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovies_dataset_transformado_final.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[7], line 22\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Extraemos las claves correspondientes y crear nuevas columnas\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys:\n\u001b[1;32m---> 22\u001b[0m         df_movies[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_movies[column_name]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnan\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Guardamos el resultado en un nuevo archivo CSV\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# df_movies.to_csv('movies_dataset_transformado_final.csv', index=False)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArchivo CSV con columnas expandidas guardado como \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovies_dataset_transformado_final.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'bool' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Repetimos el proceso con las otras columnas\n",
    "columns_to_process = {\n",
    "    'belongs_to_collection': ['id', 'name', 'poster_path', 'backdrop_path'],\n",
    "    'production_companies': ['id', 'name'],\n",
    "    'production_countries': ['iso_3166_1', 'name'],\n",
    "    'spoken_languages': ['iso_639_1', 'name']\n",
    "}\n",
    "\n",
    "# Procesamos cada columna en la lista\n",
    "for column_name, keys in columns_to_process.items():\n",
    "    # Convertimos la columna a listas de diccionarios\n",
    "    df_movies[column_name] = df_movies[column_name].apply(safe_literal_eval)\n",
    "\n",
    "    # Verificamos si hay valores inesperados en la columna\n",
    "    invalid_values = df_movies[~df_movies[column_name].apply(lambda x: isinstance(x, list))][column_name]\n",
    "    if not invalid_values.empty:\n",
    "        print(f\"Valores inesperados en la columna {column_name}:\")\n",
    "        print(invalid_values)\n",
    "    \n",
    "    # Extraemos las claves correspondientes y crear nuevas columnas\n",
    "    for key in keys:\n",
    "        df_movies[f'{column_name}_{key.upper()}'] = df_movies[column_name].apply(lambda x: [d.get(key, np.nan) for d in x if isinstance(d, dict)])\n",
    "\n",
    "# Guardamos el resultado en un nuevo archivo CSV\n",
    "# df_movies.to_csv('movies_dataset_transformado_final.csv', index=False)\n",
    "\n",
    "print(\"Archivo CSV con columnas expandidas guardado como 'movies_dataset_transformado_final.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay columnas con valores booleanos! Esto no nos permitirá desarrollar el proceso de desanidado, siendo que son tres registros (contra 45K), procederemos a eliminarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo CSV limpio guardado como 'movies_dataset_registros_borrrados.csv'\n"
     ]
    }
   ],
   "source": [
    "# Lista de índices de las filas que queremos eliminar\n",
    "indices_to_remove = [19730, 29503, 35587]\n",
    "\n",
    "# Eliminamos las filas correspondientes\n",
    "df_cleaned = df_movies.drop(indices_to_remove)\n",
    "\n",
    "# ^^^^^^^^^^^^^^^^^^Repetimos el proceso del cuadro anterior!^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos la columna original si ya no es necesaria\n",
    "df_movies.drop(columns=['belongs_to_collection','production_companies', 'production_countries', 'spoken_languages'], inplace=True)\n",
    "\n",
    "# Guardamos el DataFrame limpio en un nuevo archivo CSV\n",
    "#df_cleaned.to_csv('movies_dataset_registros_borrrados.csv', index=False)\n",
    "\n",
    "print(\"Archivo CSV limpio guardado como 'movies_dataset_registros_borrrados.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEGUIMOS CON EL DATASET CREDITS:\n",
    "# Lo separaremos en dos datasets; Crew y Cast.\n",
    "\n",
    "# CREW\n",
    "# Filtramos filas donde 'crew' tenga longitud mayor a 0\n",
    "df_valid_crew = df_credits[df_credits['crew'].str.len() > 0]\n",
    "\n",
    "# Convertimos la columna 'crew' a listas de diccionarios si es necesario\n",
    "df_valid_crew['crew'] = df_valid_crew['crew'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Expandimos diccionarios en filas separadas\n",
    "df_crew_expanded = df_valid_crew.explode('crew').reset_index(drop=True)\n",
    "\n",
    "# Convertimos los diccionarios en columnas\n",
    "df_crew_expanded = pd.json_normalize(df_crew_expanded['crew'])\n",
    "\n",
    "# Verificamos la longitud antes de repetir 'id'\n",
    "expected_length = df_crew_expanded.shape[0]\n",
    "repeated_ids = df_valid_crew.loc[df_valid_crew.index.repeat(df_valid_crew['crew'].str.len()), 'id'].values\n",
    "\n",
    "# Si la longitud no coincide, ajustar\n",
    "if len(repeated_ids) > expected_length:\n",
    "    repeated_ids = repeated_ids[:expected_length]\n",
    "elif len(repeated_ids) < expected_length:\n",
    "    df_crew_expanded = df_crew_expanded.iloc[:len(repeated_ids)]\n",
    "\n",
    "# Asignamos la columna 'id'\n",
    "df_crew_expanded['id'] = repeated_ids\n",
    "\n",
    "# Restablecemos el índice\n",
    "df_crew_expanded.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(df_crew_expanded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_crew_expanded.to_csv('C:/Users/Usuario/Desktop/Data Science/SoyHenry/PI1/df_crew_expanded.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAST\n",
    "# Filtramos filas con longitud mayor a 0\n",
    "df_valid_cast = df_credits[df_credits['cast'].str.len() > 0]\n",
    "\n",
    "# Expandimos diccionarios en filas separadas\n",
    "df_cast_expanded = df_valid_cast.explode('cast')\n",
    "\n",
    "# Convertimos los diccionarios en columnas\n",
    "df_cast_expanded = pd.json_normalize(df_cast_expanded['cast'])\n",
    "\n",
    "# Agregamos la columna 'id' del DataFrame original\n",
    "df_cast_expanded['id'] = df_valid_cast.loc[df_valid_cast.index.repeat(df_valid_cast['cast'].str.len()), 'id'].values\n",
    "\n",
    "# Restablecemos el índice\n",
    "df_cast_expanded.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(df_cast_expanded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cast_expanded.to_csv('C:/Users/Usuario/Desktop/Data Science/SoyHenry/PI1/df_cast_expanded.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Tratamiento de valores nulos:\n",
    "\n",
    "# Rellenamos los valores nulos de las columnas 'revenue' y 'budget' con 0\n",
    "df_movies['REVENUE'].fillna(0, inplace=True)\n",
    "df_movies['BUDGET'].fillna(0, inplace=True)\n",
    "\n",
    "# Eliminamos las filas donde 'release_date' es nulo\n",
    "df_movies = df_movies.dropna(subset=['RELEASE_DATE'])\n",
    "\n",
    "# 3) Tratamiento de fechas:\n",
    "\n",
    "# Aseguramos que la columna release_date esté en formato datetime\n",
    "df_movies['RELEASE_DATE'] = pd.to_datetime(df_movies['RELEASE_DATE'], errors='coerce')\n",
    "\n",
    "# Convertimos la fecha al formato AAAA-mm-dd\n",
    "df_movies['RELEASE_DATE'] = df_movies['RELEASE_DATE'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# 4) Creación de columnas nuevas:\n",
    "\n",
    "# Creamos una nueva columna release_year extrayendo el año de release_date\n",
    "df_movies['RELEASE_YEAR'] = pd.to_datetime(df_movies['RELEASE_DATE']).dt.year\n",
    "\n",
    "# Convertimos las columnas 'REVENUE' y 'BUDGET' a numéricas, forzando errores a NaN\n",
    "df_movies['REVENUE'] = pd.to_numeric(df_movies['REVENUE'], errors='coerce')\n",
    "df_movies['BUDGET'] = pd.to_numeric(df_movies['BUDGET'], errors='coerce')\n",
    "\n",
    "# Creamos la columna 'RETURN' calculando REVENUE / BUDGET\n",
    "df_movies['RETURN'] = df_movies.apply(lambda x: x['REVENUE'] / x['BUDGET'] if x['BUDGET'] > 0 else 0, axis=1)\n",
    "\n",
    "# 5) Eliminación de columnas:\n",
    "\n",
    "# Eliminamos las columnas video, imdb_id, adult, original_title, poster_path y homepage.\n",
    "df_movies.drop(columns=['video', 'imdb_id', 'adult', 'original_title', 'poster_path', 'homepage'], inplace=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
